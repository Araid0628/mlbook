\chapter{线性模型}
\linespread{1.3} 
\setlength{\parskip}{6pt}
\begin{enumerate}
	\item 线性回归原理\\
	给定数据集D，线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记：
	\begin{flalign}
		f(x_i)=\omega x_i+b,使得f(x_i)\simeq y_i\\
	\end{flalign}
	均方误差是回归任务中最常用的性能度量，因此我们可以试图让均方误差最小化。均方误差有非常好的几何意义，它对应了常用的欧式距离。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。\\
	\begin{flalign}
		(w_{*},b_{*})=\mathop{\arg\min}_{(w,b)}\sum_{i=1}^{m}(f(x_i)-y_i)^2\\
		=\mathop{\arg\min_{(w,b)}\sum_{i=1}^{m}(y_i-\omega x_i-b)^2}
	\end{flalign}
	求解$\omega$和b使得方程最小化的过程，称为线性回归模型的最小二乘参数估计。
	\item 多元线性回归原理\\
	为了便于讨论，我们把$\omega$和b吸收入向量形式$\hat{\omega}=(\omega;b)$,相应的把数据集D表示为一个$m\times(d+1)$大小的矩阵X，其中每行对应一个示例，该行前d个元素对应于示例的d个属性值。有，\\
	\begin{flalign}
		\hat{\omega}^{*}=\mathop{\arg\min_{\hat{\omega}}(y-X\hat{\omega})^T(y-X\hat{\omega})}\\
	\end{flalign}
	对$\hat{\omega}$求导得到\\
	\begin{flalign}
		\frac{\partial E_{\hat{\omega}}}{\partial \hat{\omega}}=2X^T(X\hat{\omega}-y)
	\end{flalign}
	\item 对数几率回归
	\begin{itemize}
		\item Sigmoid函数\\
		它将z值转化为一个接近0或1的y值，并且其输出值在$z=0$附近变化很陡。
		\begin{flalign}
			y=\frac{1}{1+e^{-z}}\\
		\end{flalign}
		将对数几率函数代入得到
			\begin{flalign}
				y=\frac{1}{1+e^{-(\omega^{T}x+b)}}\\
			\end{flalign}
		可变化为
			\begin{flalign}
				\ln\frac{y}{1-y}=\omega^{T}x+b\\
			\end{flalign}
	由此可看出，实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率，这种方法有很多优点，例如它是直接对分类可能性进行建模，无需事先假设分布；它不是仅预测出类别，而是可得到近似概率预测。
		\item 极大似然法
		将式重写为
			\begin{flalign}
				\ln\frac{p(y=1|x)}{p(y=0|x)}=\omega^{T}x+b\\
				p(y=1|x)=\frac{e^{\omega^{T}x+b}}{1+\omega^{T}x+b}\\
				p(y=0|x)=\frac{1}{1+\omega^{T}x+b}\\
			\end{flalign}
		于是我们可通过极大似然法来估计$\omega$和b，对数几率回归模型最大化对数似然，即令每个样本属于其真实标记的概率越大越好，连乘操作易造成下溢，通常使用对数似然。
			\begin{flalign}
				l(\omega,b)=\sum_{i=1}^{m}\ln p(y_i|x_i;\omega,b)\\
				l(\beta,b)=\sum_{i=1}^{m}\ln(y_{i}p_{1}(\hat{x_i};\beta)+(1-y_i)p_{0}(\hat{x_i};\beta)\\
				其中p1(\hat{x_i};\beta)=\frac{e^{\beta^{T}\hat{x_i}}}{1+e^{\beta^{T}\hat{x_i}}},
				p0(\hat{x_i};\beta)=\frac{1}{1+e^{\beta^{T}\hat{x_i}}}\\代入上式可得,\\
				l(\omega,b)=\sum_{i=1}^{m}\ln \frac{y_{i}e^{\beta^{T}\hat{x_i}}+1-y_{i}}{1+e^{\beta^{T}\hat{x_i}}}\\
				l(\omega,b)=\sum_{i=1}^{m}(ln(y_{i}e^{\beta^{T}\hat{x_i}}+1-y_{i})-\ln(1+e^{\beta^{T}\hat{x_i}})\\
			 	y_i=0 or 1
				l(\omega,b)=
				\sum_{i=1}^{m}(-\ln(1+e^{\beta^{T}\hat{x_i}})), \quad & y_i=0\\
				\sum_{i=1}^{m}(\beta^{T}\hat{x_i}-\ln(1+e^{\beta^{T}\hat{x_i}})), \quad & y_i=1\\
				l(\beta)=\sum_{i=1}^{m}(y_{i}\beta^{T}\hat{x_i}-\ln(1+e^{\beta^{T}\hat{x_i}}))
			\end{flalign}
				由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号即可得
			
	\end{itemize}
	\item  L0,L1,L2正则
	\begin{itemize}
		\item L0正则\\
		L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。
		\item L1正则(Lasso)\\
		L1正则化是权值的绝对值之和，是带有绝对值符号的函数，因此是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数后添加L1正则化项时，相当于对做了一个约束。令，则，此时我们的任务变成在约束下求出取最小值的解。优点：
		\begin{itemize}
			\item[-]特征选择，稀疏的矩阵可以起到特征选择的作用。我们知道，稀疏矩阵是只有少数值为非零，大多数值为0的矩阵。有些场景中特征数量可能会非常多，不利于直接使用机器学习算法。如果这是可以将参数矩阵转换稀疏的，那么就可以只保留一小部分特征（只有系数为非零的那一部分特征被保留）。这样转换成稀疏矩阵就可以起到特征选择的作用。
			\item[-] 可解释性。另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：$y=\omega_1*x_1+\omega_2*x_2+…+\omega_1000*x_1000+b$（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。
		\end{itemize}
		\item L2正则(Ridge)
		L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项$||\omega||^2$最小，可以使得$\omega$的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。
		\item L1和L2的区别
		\begin{itemize}
			\item[-] L1的下降速度比L2的下降速度要快\\
			\item[-] L1范数：L1范数在正则化的过程中会趋向于产生少量的特征，而其他的特征都是0（L1会使得参数矩阵变得稀疏）。因此L1不仅可以起到正则化的作用，还可以起到特征选择的作用。			L2范数：L2范数是通过使权重衰减，进而使得特征对于总体的影响减小而起到防止过拟合的作用的。L2的优点在于求解稳定、快速。
		\end{itemize}
		 
	\end{itemize}
\end{enumerate}