\chapter{决策树}
\setlength{\parskip}{6pt}
\begin{enumerate}
	\item 分类与回归的区别
	\begin{itemize}
		\item 回归与分类的根本区别在于输出空间是否为一个度量空间。
		\item 对于回归问题，其输出空间B是一个度量空间，即所谓“定量”。对于分类问题，其输出空间B不是度量空间，即所谓“定性”。
	\end{itemize}
	\item 决策树的特征选择
	我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度越来越高。
	\begin{itemize}
		\item ID3——最大信息增益\\
		信息熵：\\
		对于样本集合D，类别数为K，数据集D的经验熵表示为H(D)，然后计算某个特征A对于数据集D的经验条件熵H(D|A)，于是信息增益g(D,A)可以表示为二者之差，可得g(D,A)=H(D)-H(D|A)
		\begin{flalign}
		H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_{2}\frac{|C_k|}{|D|}  &\\
		H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i) &\\
		=\sum_{i=1}^{n}\frac{|D_i|}{|D|}(-\sum_{i=1}^{k}\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|} )  \\gain = H(D) - H(D|A)
		\end{flalign}
		%$H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_{2}\frac{|C_k|}{|D|}$\\  
		%$H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{D}H(D_i)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}(-\s%um_{i=1}^{k}\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|} )$\\  
		%$gain = H(D) - H(D|A)$\\  
		\item C4.5——增益率
		特征A对数据集D的信息增益比定义为：\\
		\begin{flalign}
			g_R=(D,A)=\frac{g(D,A)}{H_A(D)}
		\end{flalign}
		其中，
		\begin{flalign}
		H_{A}(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
		\end{flalign}
		\item CART——基尼系数
		Gini描述的是数据的纯度，与信息熵含义类似。
		\begin{flalign}
		Gini(D)=1-\sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2
		\end{flalign}
		与ID3,C4.5不同的是，CART是一颗二叉树，采用二元分割法，每一步将数据按特征A的取值切分成两份，分别进入左右子树。对于样本D，如根据特征A的某个值A，把D分为D1和D2两部分，则在特征A的条件下，特征A的Gini指数定义为：
		\begin{flalign}
		Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)
		\end{flalign}
		基尼系数与熵之半的曲线非常接近，因此基尼系数可以做为熵模型的一个近似替代。
	\end{itemize}
	\item 随机森林\\
	随机森林是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。
	\begin{itemize}
		\item 随机性1\\
		在训练时，随机森林中的每棵树都会从数据点的随机样本中学习。样本被有放回的抽样，称为自助抽样法（bootstrapping），这意味着一些样本将在一棵树中被多次使用。背后的想法是在不同样本上训练每棵树，尽管每棵树相对于特定训练数据集可能具有高方差，但总体而言，整个森林将具有较低的方差，同时不以增加偏差为代价。\\
		\item 随机性2\\
		随机森林中的另一个主要概念是，只考虑所有特征的一个子集来拆分每个决策树中的每个节点。通常将其设置为sqrt以进行分类，这意味着如果有16个特征，则在每个树中的每个节点处，只考虑4个随机特征来拆分节点。\\
	\end{itemize}
	\item 随机森林优缺点
	优点：
	\begin{itemize}		
		\item[-] 随机森林抗过拟合能力比较强
		\item[-] 随机森林算法有很强的抗干扰能力（数据存在大量缺失）
		\item[-] 训练速度快，
		\item[-] 由于采用了随机采样，训练出的模型的方差小，泛化能力强。
	\end{itemize}
	缺点：
	\begin{itemize}	
		\item[-] 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
		\item[-] 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
	\end{itemize}
	\item 剪枝
	\begin{enumerate}
		\item 决策树的损失函数\\
		根据叶结点中的预测误差来衡量，即训练数据的拟合程度。考虑到所有的叶结点中每个叶节点中的样例个数不同，我们采用
		\begin{flalign}
		C(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log\frac{N_{tk}}{N_t}
		\end{flalign}
		来衡量模型对训练数据的整体误差。但是如果仅仅使用$C(T)$来作为优化目标函数，就会导致模型走向过拟合的结果。\\
		为了避免过拟合，我们需要给优化目标函数增加一个正则项，正则项应该包含模型的复杂度信息。对于决策树来说，其叶节点的数量$|T|$越多就越复杂，我们用添加正则项的
		\begin{flalign}
			C_\alpha(T)=C(T)+\alpha|T|
		\end{flalign}
		来作为优化的目标函数，也就是树的损失函数。参数$\alpha$控制了两者之间的影响程度，较大的$\alpha$促使选择较简单的模型，较小的$\alpha$促使选择复杂的模型。为了提高决策树的泛化能力，需要对树进行 剪枝 (Pruning)，把过于细分的叶结点（通常是数据量过少导致噪声数据的影响增加）去掉而回退到其父结点或更高的结点，使其父结点或更高的结点变为叶结点。\\
		剪枝的目的不是为了最小化损失函数，剪枝的目的是为了达到一个更好的泛化能力。而对于决策树来说，叶结点的数量越多，反应了决策树对训练数据的细节反应的越多，继而弱化了泛化能力。
		\item 预剪枝
		通过提前停止树的构建而对树剪枝，一旦停止，节点就是树叶，该树叶持有子集元祖最频繁的类。	
		停止决策树生长最简单的方法有：
		1.定义一个高度，当决策树达到该高度时就停止决策树的生长
		2.达到某个节点的实例具有相同的特征向量，及时这些实例不属于同一类，也可以停止决策树的生长。这个方法对于处理数据的数据冲突问题比较有效。		
		3.定义一个阈值，当达到某个节点的实例个数小于阈值时就可以停止决策树的生长	
		4.定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值大小来决定是否停止决策树的生长。
		\item 后剪枝
		它首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。
		\begin{enumerate}
			\item REP 错误率降低剪枝
			对于完全决策树中的每一个非叶子节点的子树，我们尝试着把它替换成一个叶子节点，该叶子节点的类别我们用子树所覆盖训练样本中存在最多的那个类来代替，这样就产生了一个简化决策树，然后比较这两个决策树在测试数据集中的表现，如果简化决策树在测试数据集中的错误比较少，那么该子树就可以替换成叶子节点。该算法以bottom-up的方式遍历所有的子树，直至没有任何子树可以替换使得测试数据集的表现得以改进时，算法就可以终止。
			\item PEP 悲观剪枝
		\end{enumerate}
	\end{enumerate}
	\item GBDT
	\begin{enumerate}
		\item GDBT概述
		GBDT也是集成学习Boosting家族的成员，但是却和传统的Adaboost有很大的不同。回顾下Adaboost，我们是利用前一轮迭代弱学习器的误差率来更新训练集的权重，这样一轮轮的迭代下去。GBDT也是迭代，使用了前向分布算法，但是弱学习器限定了只能使用CART回归树模型，同时迭代思路和Adaboost也有所不同。\\
		在GBDT的迭代中，假设我们前一轮迭代得到的强学习器是$f_{t-1}(x)$,损失函数是$L(y,f_{t-1}(x))$,我们本轮迭代的目标是找到一个CART回归树模型的弱学习器$h_{t}(x)$,让本轮损失函数$L(y,f_{t}(x))=L(y,f_{t-1}(x)+h_{t}(x)$最小。
		\item GDBT原理
		
		\item GBDT的优缺点
		优点:\\
		\begin{itemize}
			\item 预测阶段计算速度快，树与树之间可并行化计算
			\item 在分布稠密的数据集上，泛化能力和表达能力都很好
			\item 具有较好的解释性和鲁棒性
		\end{itemize}
		缺点:\\
		\begin{itemize}
			\item 在高维稀疏的数据集上，表现不如SVM或NN
			\item 在处理文本分类特征问题上优势不大
			\item 训练过程需要串行训练
		\end{itemize}
	\end{enumerate}
\end{enumerate}