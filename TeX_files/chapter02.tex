\chapter{SVM}
\linespread{1.3} 
\setlength{\parskip}{6pt}
\begin{enumerate}
	\item 原理\\
	给定训练样本集，分类学习最基本的想法就是基于训练集D再样本空间中找打一个划分超平面，将不同类别的样本分开。\\
	在样本空间中，划分超平面可通过如下线性方程来描述：
	\begin{flalign}
		\omega^\mathrm{T}x+b=0
	\end{flalign}
	其中$\omega$为法向量，决定了超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。\\
	样本中任意点x到超平面$(w,b)$的距离可写为
	\begin{center}
		\begin{flalign}
			r=\frac{\omega^\mathrm{T}x+b}{||\omega||}
		\end{flalign}
	\end{center}
	假设超平面能将训练样本正确分类，
	\begin{flalign}
	\omega^\mathrm{T}x+b \geq +1, y_i=+1 \\
	\omega^\mathrm{T}x+b \leq -1, y_i=-1
	\end{flalign}
	距离超平面最近的这几个训练样本点使等号成立，它们被称为支持向量，两个支持向量到超平面的距离为\\
	\begin{flalign}
		\gamma = \frac{2}{||\omega||}
	\end{flalign}
	它被称为间隔。欲找到具有最大间隔的划分平面也就是
	\begin{flalign}
		max_{\omega ,b} \frac{2}{||\omega||} \\
		s.t. y_i(\omega^\mathrm{T}x+b)\geq +1
	\end{flalign}
	最大化$||\omega||^{-1}$，这等价于最小化$||\omega||^{2}$，于是可重写为$min_{\omega ,b} \frac{1}{2} ||w||^{2}$
	\item 对偶问题\\
	原问题本身是一个凸二次规划问题（目标函数是二次的，约束条件是线性的），能直接用现成的优化计算报求解，但我们有更高效的办法。对式使用拉格朗日乘子法可得到其对偶问题。
	\begin{flalign}
		L(\omega,b,\alpha) = \frac{1}{2}||w||^2+\sum_{m}^{i=1}\alpha_{i}(1-y_i(\omega^\mathrm{T}x+b))
	\end{flalign}
	令$L(\omega,b,\alpha)$对$\omega$ 和b的偏导为零可得
	\begin{flalign}
		\omega=\sum_{i=1}^{m}\alpha_{i}y_{i}x_{i},\\
		0=\sum_{i=1}^{m}\alpha_{i}y_{i}
	\end{flalign}
	代入$L(\omega,b,\alpha)$中，
	\begin{flalign}
		L(\omega,b,\alpha)=\frac{1}{2}\omega\omega^\mathrm{T}+\sum_{m}^{i=1}\alpha_i-b\sum_{i=1}^{m}\alpha_{i}y_i-\omega^\mathrm{T}\sum_{i=1}^{m}\alpha_{i}x_{i}y_{i}
	\end{flalign} $=\frac{1}{2}\omega\omega^\mathrm{T}+\sum_{i=1}^{m}\alpha_i-b\cdot0-\omega^\mathrm{T}\sum_{i=1}^{m}\alpha_{i}x_{i}y_{i}$\\
		\qquad\qquad $=\frac{1}{2}\omega^\mathrm{T}\sum_{i=1}^{m}\alpha_{i}x_{i}y_{i}+\sum_{m}^{i=1}\alpha_i-\omega^\mathrm{T}\sum_{i=1}^{m}\alpha_{i}x_{i}y_{i}$\\
		\qquad\qquad\quad $=\sum_{m}^{i=1}\alpha_i-\frac{1}{2}(\sum_{i=1}^{m}\alpha_{i}x_{i}y_{i})^\mathrm{T}(\sum_{i=1}^{m}\alpha_{i}x_{i}y_{i})$\\
		\qquad\qquad\quad $=\sum_{m}^{i=1}\alpha_i-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}x_{i}^\mathrm{T}x_{j}y_{i}y_{j}$\\
		\qquad\qquad\quad $\alpha_i \geq 0, \sum_{i=1}^{m}\alpha_{i}y_{i}=0$\\
	\item KKT条件
	\begin{itemize}
		\item 为什么转换成对偶问题：\\			
		1.对偶问题将原始问题中的约束转为了对偶问题中的等式约束\\
		2.方便核函数的引入\\
		3.改变了问题的复杂度。由求特征向量$\omega$转化为求比例系数$\alpha$，在原始问题下，求解的复杂度与样本的维度有关，即$\omega$的维度。在对偶问题下，只与样本数量有关。\\
		\item KKT条件：\\
		1.\quad $\alpha_{i} \geq 0$\\
		2.\quad $y_{i}(\omega^\mathrm{T}x+b-1) \geq 0$\\
		3.\quad $\sum\alpha_{i}(y_{i}(\omega^\mathrm{T}x+b-1))=0$\\
	\end{itemize}
	\item 核函数
	\begin{itemize}
		\item 高斯核为什么有效？\\
		在现实任务中，原始样本空间内也许并不存在一个能正确划分两类样本的超平面。对这样的问题，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。令$\phi(x)$表示将$x$映射后的特征向量，于是，在特征空间中划分超平面所对应的模型可表示为$f(x)=\omega^\mathrm{T}\phi(x)+b$
		\item 常用的核函数\\
		1.线性核\\
		2.多项式核\\
		3.高斯核\\
		4.拉普拉斯核\\
		5.Sigmoid核\\
	\end{itemize}
	\item 过拟合
	\begin{itemize}
		\item 松弛变量$\xi$\\
		约束条件变为:$s.t. y_i(\omega^\mathrm{T}x+b)\geq 1-\xi_i$\\
		引入松弛变量使SVM能够容忍异常点的存在。为什么？因为引入松弛变量后，所有点到超平面的距离约束不需要大于等于1了，而是大于0.8就行了（如果ξ=0.2的话），那么异常点就可以不是支持向量了，它就作为一个普通的点存在，我们的支持向量和超平面都不会受到它的影响。
		\item 软间隔支持向量机、、
		在最大化间隔的同时，不满足约束的样本应尽可能少。
		\begin{flalign}
			min_{\omega,b,\xi_i} \frac{1}{2} ||w||^{2}+C\sum_{i=1}^{m}\xi_i
		\end{flalign}	
	\end{itemize}
\end{enumerate}
